{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "* 嘗試比較用 color histogram 和 HOG 特徵來訓練的 SVM 分類器在 cifar10 training 和 testing data 上準確度的差別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import cv2 # 載入 cv2 套件\n",
    "import matplotlib.pyplot as plt\n",
    "from liblinear import liblinearutil as svm\n",
    "\n",
    "train, test = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "y_train = y_train.astype(int).reshape(-1)\n",
    "y_test = y_test.astype(int).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生直方圖特徵的訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_histogram = []\n",
    "x_test_histogram = []\n",
    "\n",
    "# 對於所有訓練資料\n",
    "for i in range(len(x_train)):\n",
    "    chans = cv2.split(x_train[i]) # 把圖像的 3 個 channel 切分出來\n",
    "    # 對於所有 channel\n",
    "    hist_feature = []\n",
    "    for chan in chans:\n",
    "        # 計算該 channel 的直方圖\n",
    "        hist = cv2.calcHist([chan], [0], None, [16], [0, 256]) # 切成 16 個 bin\n",
    "        hist_feature.extend(hist.flatten())\n",
    "    # 把計算的直方圖特徵收集起來\n",
    "    x_train_histogram.append(hist_feature)\n",
    "\n",
    "# 對於所有測試資料也做一樣的處理\n",
    "for i in range(len(x_test)):\n",
    "    chans = cv2.split(x_test[i]) # 把圖像的 3 個 channel 切分出來\n",
    "    # 對於所有 channel\n",
    "    hist_feature = []\n",
    "    for chan in chans:\n",
    "        # 計算該 channel 的直方圖\n",
    "        hist = cv2.calcHist([chan], [0], None, [16], [0, 256]) # 切成 16 個 bin\n",
    "        hist_feature.extend(hist.flatten())\n",
    "    x_test_histogram.append(hist_feature)\n",
    "\n",
    "x_train_histogram = np.array(x_train_histogram)\n",
    "x_test_histogram = np.array(x_test_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生 HOG 特徵的訓練資料\n",
    "* HOG 特徵通過計算和統計圖像局部區域的梯度方向直方圖來構建特徵，具體細節不在我們涵蓋的範圍裡面，有興趣的同學請參考[補充資料](https://www.cnblogs.com/zyly/p/9651261.html)哦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_n = 16 # Number of bins\n",
    "\n",
    "def hog(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gx = cv2.Sobel(img, cv2.CV_32F, 1, 0)\n",
    "    gy = cv2.Sobel(img, cv2.CV_32F, 0, 1)\n",
    "    mag, ang = cv2.cartToPolar(gx, gy)\n",
    "    bins = np.int32(bin_n * ang / (2 * np.pi))    # quantizing binvalues in (0...16)\n",
    "    bin_cells = bins[:10, :10], bins[10:, :10], bins[:10, 10:], bins[10:, 10:]\n",
    "    mag_cells = mag[:10, :10], mag[10:, :10], mag[:10, 10:], mag[10:, 10:]\n",
    "    hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n",
    "    hist = np.hstack(hists)     # hist is a 64 bit vector\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "x_train_hog = np.array([hog(x) for x in x_train])\n",
    "x_test_hog = np.array([hog(x) for x in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM model\n",
    "* SVM 是機器學習中一個經典的分類算法，具體細節有興趣可以參考 [該知乎上的解釋](https://www.zhihu.com/question/21094489)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module liblinear.liblinearutil:\n",
      "\n",
      "train(arg1, arg2=None, arg3=None)\n",
      "    train(y, x [, options]) -> model | ACC\n",
      "    \n",
      "    y: a list/tuple/ndarray of l true labels (type must be int/double).\n",
      "    \n",
      "    x: 1. a list/tuple of l training instances. Feature vector of\n",
      "          each training instance is a list/tuple or dictionary.\n",
      "    \n",
      "       2. an l * n numpy ndarray or scipy spmatrix (n: number of features).\n",
      "    \n",
      "    train(prob [, options]) -> model | ACC\n",
      "    train(prob, param) -> model | ACC\n",
      "    \n",
      "    Train a model from data (y, x) or a problem prob using\n",
      "    'options' or a parameter param.\n",
      "    \n",
      "    If '-v' is specified in 'options' (i.e., cross validation)\n",
      "    either accuracy (ACC) or mean-squared error (MSE) is returned.\n",
      "    \n",
      "    options:\n",
      "        -s type : set type of solver (default 1)\n",
      "          for multi-class classification\n",
      "             0 -- L2-regularized logistic regression (primal)\n",
      "             1 -- L2-regularized L2-loss support vector classification (dual)\n",
      "             2 -- L2-regularized L2-loss support vector classification (primal)\n",
      "             3 -- L2-regularized L1-loss support vector classification (dual)\n",
      "             4 -- support vector classification by Crammer and Singer\n",
      "             5 -- L1-regularized L2-loss support vector classification\n",
      "             6 -- L1-regularized logistic regression\n",
      "             7 -- L2-regularized logistic regression (dual)\n",
      "          for regression\n",
      "            11 -- L2-regularized L2-loss support vector regression (primal)\n",
      "            12 -- L2-regularized L2-loss support vector regression (dual)\n",
      "            13 -- L2-regularized L1-loss support vector regression (dual)\n",
      "          for outlier detection\n",
      "            21 -- one-class support vector machine (dual)\n",
      "        -c cost : set the parameter C (default 1)\n",
      "        -p epsilon : set the epsilon in loss function of SVR (default 0.1)\n",
      "        -e epsilon : set tolerance of termination criterion\n",
      "            -s 0 and 2\n",
      "                |f'(w)|_2 <= eps*min(pos,neg)/l*|f'(w0)|_2,\n",
      "                where f is the primal function, (default 0.01)\n",
      "            -s 11\n",
      "                |f'(w)|_2 <= eps*|f'(w0)|_2 (default 0.0001)\n",
      "            -s 1, 3, 4, 7, and 21\n",
      "                Dual maximal violation <= eps; similar to libsvm (default 0.1 except 0.01 for -s 21)\n",
      "            -s 5 and 6\n",
      "                |f'(w)|_inf <= eps*min(pos,neg)/l*|f'(w0)|_inf,\n",
      "                where f is the primal function (default 0.01)\n",
      "            -s 12 and 13\n",
      "                |f'(alpha)|_1 <= eps |f'(alpha0)|,\n",
      "                where f is the dual function (default 0.1)\n",
      "        -B bias : if bias >= 0, instance x becomes [x; bias]; if < 0, no bias term added (default -1)\n",
      "        -R : not regularize the bias; must with -B 1 to have the bias; DON'T use this unless you know what it is\n",
      "            (for -s 0, 2, 5, 6, 11)\"\n",
      "        -wi weight: weights adjust the parameter C of different classes (see README for details)\n",
      "        -v n: n-fold cross validation mode\n",
      "        -C : find parameters (C for -s 0, 2 and C, p for -s 11)\n",
      "        -q : quiet mode (no outputs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(svm.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用 histogram 特徵訓練 SVM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Accuracy ===\n",
      "Accuracy = 25.424% (12712/50000) (classification)\n",
      "=== Testing Accuracy ===\n",
      "Accuracy = 25.25% (2525/10000) (classification)\n"
     ]
    }
   ],
   "source": [
    "SVM_hist = svm.train(y_train, x_train_histogram, [\"-s\", 1, \"-c\", 1])\n",
    "\n",
    "# prediction\n",
    "print(\"=== Training Accuracy ===\")\n",
    "_ = svm.predict(y_train, x_train_histogram, SVM_hist)\n",
    "print(\"=== Testing Accuracy ===\")\n",
    "_ = svm.predict(y_test, x_test_histogram, SVM_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用 HOG 特徵訓練 SVM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Accuracy ===\n",
      "Accuracy = 39.156% (19578/50000) (classification)\n",
      "=== Testing Accuracy ===\n",
      "Accuracy = 38.54% (3854/10000) (classification)\n"
     ]
    }
   ],
   "source": [
    "SVM_hog = svm.train(y_train, x_train_hog, [\"-s\", 1, \"-c\", 1])\n",
    "\n",
    "# prediction\n",
    "print(\"=== Training Accuracy ===\")\n",
    "_ = svm.predict(y_train, x_train_hog, SVM_hog)\n",
    "print(\"=== Testing Accuracy ===\")\n",
    "_ = svm.predict(y_test, x_test_hog, SVM_hog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
